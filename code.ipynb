{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438ac43a-9a0a-4f0f-8973-1e18e5955816",
      "metadata": {
        "id": "438ac43a-9a0a-4f0f-8973-1e18e5955816",
        "outputId": "5457be9d-7802-4f48-d601-0f276bc6d88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82890904-a708-416b-9190-23c0f4ec0577",
      "metadata": {
        "id": "82890904-a708-416b-9190-23c0f4ec0577",
        "outputId": "d0035aee-6a24-40b0-94d6-f3b49ac771da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amazon_Reviews Spark Session Started üöÄ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/04 16:43:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "# Set Spark environment variables\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/iiitdmk-sic40/spark-3.5.5\"  # Update if different\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
        "\n",
        "# Initialize findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"spark://172.16.72.48:7077\") \\\n",
        "    .appName(\"Amazon_Reviews\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Amazon_Reviews Spark Session Started üöÄ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550821c0-b245-4087-b704-986f6a324769",
      "metadata": {
        "id": "550821c0-b245-4087-b704-986f6a324769"
      },
      "outputs": [],
      "source": [
        "#Required Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb48c18-c366-494b-b432-41b0a7354b57",
      "metadata": {
        "id": "9bb48c18-c366-494b-b432-41b0a7354b57",
        "outputId": "17593fca-3d9f-4792-a09a-072b67f1372e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
            "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
            "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
            "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n",
            "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n",
            "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n",
            "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n",
            "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"hdfs://localhost:9000/user/iiitdmk/dataset/Amazon_reviews.csv\", header=True, inferSchema=True)\n",
        "df.show(5)  # Display the first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af8f700-842c-4b28-aef8-7df2bc15a3d9",
      "metadata": {
        "id": "6af8f700-842c-4b28-aef8-7df2bc15a3d9",
        "outputId": "8a3d25da-0b16-4ff2-c948-a85afe18f706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in ./.local/lib/python3.8/site-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e0919c-242c-4a36-8af0-cd519ddaf621",
      "metadata": {
        "id": "b3e0919c-242c-4a36-8af0-cd519ddaf621",
        "outputId": "7d7f91e6-c2d3-48c0-df56-1e843b0a394c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/iiitdmk-\n",
            "[nltk_data]     sic27/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/iiitdmk-\n",
            "[nltk_data]     sic27/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/iiitdmk-\n",
            "[nltk_data]     sic27/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd03adb-e3f5-4c78-8a7b-6d1a8791d416",
      "metadata": {
        "id": "dfd03adb-e3f5-4c78-8a7b-6d1a8791d416"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Set Up PySpark for Jupyter Notebook\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70f6f3d-a80e-4a10-bcbe-75a8bdd97d5b",
      "metadata": {
        "id": "e70f6f3d-a80e-4a10-bcbe-75a8bdd97d5b"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15bb4e68-931a-4302-b698-02ed8f8df777",
      "metadata": {
        "id": "15bb4e68-931a-4302-b698-02ed8f8df777",
        "outputId": "1011d6b1-1688-47b7-9443-fc5ad15eda1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/04 16:44:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Start Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Amazon_Reviews\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e371b035-b606-4492-8eed-4985ce35c637",
      "metadata": {
        "id": "e371b035-b606-4492-8eed-4985ce35c637",
        "outputId": "c5e7b91f-adcb-452e-db4b-c06ab5d4c41a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[Id: int, ProductId: string, UserId: string, ProfileName: string, HelpfulnessNumerator: string, HelpfulnessDenominator: string, Score: string, Time: string, Summary: string, Text: string]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ‚úÖ Load Data from HDFS\n",
        "df = spark.read.csv(\"hdfs://localhost:9000/user/iiitdmk/dataset/Amazon_reviews.csv\", header=True, inferSchema=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecaadb0-402c-4ea8-b54f-16ab9d42f43a",
      "metadata": {
        "id": "4ecaadb0-402c-4ea8-b54f-16ab9d42f43a",
        "outputId": "772038a5-690b-4d38-fabb-a506f6aa3565"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Id   ProductId          UserId                          ProfileName  \\\n",
            "0   1  B001E4KFG0  A3SGXH7AUHU8GW                           delmartian   \n",
            "1   2  B00813GRG4  A1D87F6ZCVE5NK                               dll pa   \n",
            "2   3  B000LQOCH0   ABXLMWJIXXAIN  \"Natalia Corres \"\"Natalia Corres\"\"\"   \n",
            "3   4  B000UA0QIQ  A395BORC6FGVXV                                 Karl   \n",
            "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    \"Michael D. Bigham \"\"M. Wassir\"\"\"   \n",
            "\n",
            "  HelpfulnessNumerator HelpfulnessDenominator Score        Time  \\\n",
            "0                    1                      1     5  1303862400   \n",
            "1                    0                      0     1  1346976000   \n",
            "2                    1                      1     4  1219017600   \n",
            "3                    3                      3     2  1307923200   \n",
            "4                    0                      0     5  1350777600   \n",
            "\n",
            "                     Summary  \\\n",
            "0      Good Quality Dog Food   \n",
            "1          Not as Advertised   \n",
            "2  \"\"\"Delight\"\" says it all\"   \n",
            "3             Cough Medicine   \n",
            "4                Great taffy   \n",
            "\n",
            "                                                Text  \n",
            "0  I have bought several of the Vitality canned d...  \n",
            "1  \"Product arrived labeled as Jumbo Salted Peanu...  \n",
            "2  \"This is a confection that has been around a f...  \n",
            "3  If you are looking for the secret ingredient i...  \n",
            "4  Great taffy at a great price.  There was a wid...  \n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Convert only if df is a PySpark DataFrame\n",
        "if isinstance(df, pyspark.sql.dataframe.DataFrame):\n",
        "    df = df.toPandas()\n",
        "\n",
        "print(df.head())  # Check if data is loaded correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b33713e-6545-4910-901b-8cefd7654fac",
      "metadata": {
        "id": "6b33713e-6545-4910-901b-8cefd7654fac",
        "outputId": "94561597-886c-4c25-ea61-4ffae85008e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[Id: int, ProductId: string, UserId: string, ProfileName: string, HelpfulnessNumerator: string, HelpfulnessDenominator: string, Score: string, Time: string, Summary: string, Text: string]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b3c5b4-619b-441c-bc19-0f46fd090843",
      "metadata": {
        "id": "95b3c5b4-619b-441c-bc19-0f46fd090843",
        "outputId": "aa0ad27b-aff3-4ac4-f073-f6762c17ee67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Convert to Pandas\n",
        "df_pandas = df.toPandas()  # Convert entire DataFrame to Pandas\n",
        "\n",
        "# ‚úÖ Train-Test Split\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_pandas['Text'], df_pandas['Score'], test_size=0.3, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ee2f59-b110-4f4f-8cdf-38eaced64ff8",
      "metadata": {
        "id": "79ee2f59-b110-4f4f-8cdf-38eaced64ff8",
        "outputId": "990a4383-c2b4-493d-edb6-9307511b9d8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/iiitdmk-\n",
            "[nltk_data]     sic27/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/iiitdmk-\n",
            "[nltk_data]     sic27/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/tmp/ipykernel_26027/3780152681.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "/tmp/ipykernel_26027/3780152681.py:10: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Text Preprocessing\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    review = soup.get_text()\n",
        "    review = re.sub(r'[^a-zA-Z]', ' ', review).lower().split()\n",
        "    review = [stemmer.stem(lemmatizer.lemmatize(word)) for word in review if word not in stop_words]\n",
        "    return ' '.join(review)\n",
        "\n",
        "corpus_train = [preprocess_text(text) for text in x_train]\n",
        "corpus_test = [preprocess_text(text) for text in x_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3485d93-ccaa-4608-b35e-3f605000420e",
      "metadata": {
        "id": "d3485d93-ccaa-4608-b35e-3f605000420e",
        "outputId": "0a4d0d61-ea62-4cc6-9414-bc9e3801e258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TF-IDF transformation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ‚úÖ Define Text Column\n",
        "text_column = \"Text\"  # Make sure this column exists in your DataFrame\n",
        "\n",
        "# ‚úÖ Ensure Data Exists\n",
        "if text_column not in df.columns:\n",
        "    raise ValueError(f\"‚ùå ERROR: Column '{text_column}' not found in DataFrame!\")\n",
        "\n",
        "# ‚úÖ Fill missing text values\n",
        "df[text_column] = df[text_column].fillna(\"\")\n",
        "\n",
        "# ‚úÖ Split Data into Train & Test\n",
        "corpus_train, corpus_test = train_test_split(df[text_column], test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚úÖ Apply TF-IDF Vectorization\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)\n",
        "tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\n",
        "tfidf_vec_test = tfidf_vec.transform(corpus_test)\n",
        "\n",
        "print(\"‚úÖ TF-IDF transformation completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d41f4cd-49c6-4c86-8486-abdb232ed765",
      "metadata": {
        "id": "9d41f4cd-49c6-4c86-8486-abdb232ed765",
        "outputId": "fe92dbbf-7352-4b9d-ef90-2ab65e5cccc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Logistic Regression Accuracy: 0.7419\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Logistic Regression (handles large sparse data better)\n",
        "log_reg = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
        "log_reg.fit(tfidf_vec_train, y_train)\n",
        "\n",
        "# ‚úÖ Predictions\n",
        "predictions = log_reg.predict(tfidf_vec_test)\n",
        "\n",
        "# ‚úÖ Accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"üéØ Logistic Regression Accuracy: {accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}